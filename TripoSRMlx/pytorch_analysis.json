{
  "model_name": "TripoSR",
  "analysis_timestamp": "2025-09-19 20:07:24.059254",
  "pytorch_parameters": {
    "image_tokenizer.model.embeddings.cls_token": {
      "shape": [
        1,
        1,
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 3
    },
    "image_tokenizer.model.embeddings.position_embeddings": {
      "shape": [
        1,
        197,
        768
      ],
      "dtype": "torch.float32",
      "size": 151296,
      "ndim": 3
    },
    "image_tokenizer.model.embeddings.patch_embeddings.projection.weight": {
      "shape": [
        768,
        3,
        16,
        16
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 4
    },
    "image_tokenizer.model.embeddings.patch_embeddings.projection.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.0.attention.attention.query.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.0.attention.attention.query.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.0.attention.attention.key.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.0.attention.attention.key.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.0.attention.attention.value.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.0.attention.attention.value.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.0.attention.output.dense.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.0.attention.output.dense.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.0.intermediate.dense.weight": {
      "shape": [
        3072,
        768
      ],
      "dtype": "torch.float32",
      "size": 2359296,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.0.intermediate.dense.bias": {
      "shape": [
        3072
      ],
      "dtype": "torch.float32",
      "size": 3072,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.0.output.dense.weight": {
      "shape": [
        768,
        3072
      ],
      "dtype": "torch.float32",
      "size": 2359296,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.0.output.dense.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.0.layernorm_before.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.0.layernorm_before.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.0.layernorm_after.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.0.layernorm_after.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.1.attention.attention.query.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.1.attention.attention.query.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.1.attention.attention.key.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.1.attention.attention.key.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.1.attention.attention.value.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.1.attention.attention.value.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.1.attention.output.dense.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.1.attention.output.dense.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.1.intermediate.dense.weight": {
      "shape": [
        3072,
        768
      ],
      "dtype": "torch.float32",
      "size": 2359296,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.1.intermediate.dense.bias": {
      "shape": [
        3072
      ],
      "dtype": "torch.float32",
      "size": 3072,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.1.output.dense.weight": {
      "shape": [
        768,
        3072
      ],
      "dtype": "torch.float32",
      "size": 2359296,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.1.output.dense.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.1.layernorm_before.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.1.layernorm_before.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.1.layernorm_after.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.1.layernorm_after.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.2.attention.attention.query.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.2.attention.attention.query.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.2.attention.attention.key.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.2.attention.attention.key.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.2.attention.attention.value.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.2.attention.attention.value.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.2.attention.output.dense.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.2.attention.output.dense.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.2.intermediate.dense.weight": {
      "shape": [
        3072,
        768
      ],
      "dtype": "torch.float32",
      "size": 2359296,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.2.intermediate.dense.bias": {
      "shape": [
        3072
      ],
      "dtype": "torch.float32",
      "size": 3072,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.2.output.dense.weight": {
      "shape": [
        768,
        3072
      ],
      "dtype": "torch.float32",
      "size": 2359296,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.2.output.dense.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.2.layernorm_before.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.2.layernorm_before.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.2.layernorm_after.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.2.layernorm_after.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.3.attention.attention.query.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.3.attention.attention.query.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.3.attention.attention.key.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.3.attention.attention.key.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.3.attention.attention.value.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.3.attention.attention.value.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.3.attention.output.dense.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.3.attention.output.dense.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.3.intermediate.dense.weight": {
      "shape": [
        3072,
        768
      ],
      "dtype": "torch.float32",
      "size": 2359296,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.3.intermediate.dense.bias": {
      "shape": [
        3072
      ],
      "dtype": "torch.float32",
      "size": 3072,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.3.output.dense.weight": {
      "shape": [
        768,
        3072
      ],
      "dtype": "torch.float32",
      "size": 2359296,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.3.output.dense.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.3.layernorm_before.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.3.layernorm_before.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.3.layernorm_after.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.3.layernorm_after.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.4.attention.attention.query.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.4.attention.attention.query.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.4.attention.attention.key.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.4.attention.attention.key.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.4.attention.attention.value.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.4.attention.attention.value.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.4.attention.output.dense.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.4.attention.output.dense.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.4.intermediate.dense.weight": {
      "shape": [
        3072,
        768
      ],
      "dtype": "torch.float32",
      "size": 2359296,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.4.intermediate.dense.bias": {
      "shape": [
        3072
      ],
      "dtype": "torch.float32",
      "size": 3072,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.4.output.dense.weight": {
      "shape": [
        768,
        3072
      ],
      "dtype": "torch.float32",
      "size": 2359296,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.4.output.dense.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.4.layernorm_before.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.4.layernorm_before.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.4.layernorm_after.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.4.layernorm_after.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.5.attention.attention.query.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.5.attention.attention.query.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.5.attention.attention.key.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.5.attention.attention.key.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.5.attention.attention.value.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.5.attention.attention.value.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.5.attention.output.dense.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.5.attention.output.dense.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.5.intermediate.dense.weight": {
      "shape": [
        3072,
        768
      ],
      "dtype": "torch.float32",
      "size": 2359296,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.5.intermediate.dense.bias": {
      "shape": [
        3072
      ],
      "dtype": "torch.float32",
      "size": 3072,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.5.output.dense.weight": {
      "shape": [
        768,
        3072
      ],
      "dtype": "torch.float32",
      "size": 2359296,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.5.output.dense.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.5.layernorm_before.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.5.layernorm_before.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.5.layernorm_after.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.5.layernorm_after.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.6.attention.attention.query.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.6.attention.attention.query.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.6.attention.attention.key.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.6.attention.attention.key.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.6.attention.attention.value.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.6.attention.attention.value.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.6.attention.output.dense.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.6.attention.output.dense.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.6.intermediate.dense.weight": {
      "shape": [
        3072,
        768
      ],
      "dtype": "torch.float32",
      "size": 2359296,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.6.intermediate.dense.bias": {
      "shape": [
        3072
      ],
      "dtype": "torch.float32",
      "size": 3072,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.6.output.dense.weight": {
      "shape": [
        768,
        3072
      ],
      "dtype": "torch.float32",
      "size": 2359296,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.6.output.dense.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.6.layernorm_before.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.6.layernorm_before.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.6.layernorm_after.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.6.layernorm_after.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.7.attention.attention.query.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.7.attention.attention.query.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.7.attention.attention.key.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.7.attention.attention.key.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.7.attention.attention.value.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.7.attention.attention.value.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.7.attention.output.dense.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.7.attention.output.dense.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.7.intermediate.dense.weight": {
      "shape": [
        3072,
        768
      ],
      "dtype": "torch.float32",
      "size": 2359296,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.7.intermediate.dense.bias": {
      "shape": [
        3072
      ],
      "dtype": "torch.float32",
      "size": 3072,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.7.output.dense.weight": {
      "shape": [
        768,
        3072
      ],
      "dtype": "torch.float32",
      "size": 2359296,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.7.output.dense.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.7.layernorm_before.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.7.layernorm_before.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.7.layernorm_after.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.7.layernorm_after.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.8.attention.attention.query.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.8.attention.attention.query.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.8.attention.attention.key.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.8.attention.attention.key.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.8.attention.attention.value.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.8.attention.attention.value.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.8.attention.output.dense.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.8.attention.output.dense.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.8.intermediate.dense.weight": {
      "shape": [
        3072,
        768
      ],
      "dtype": "torch.float32",
      "size": 2359296,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.8.intermediate.dense.bias": {
      "shape": [
        3072
      ],
      "dtype": "torch.float32",
      "size": 3072,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.8.output.dense.weight": {
      "shape": [
        768,
        3072
      ],
      "dtype": "torch.float32",
      "size": 2359296,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.8.output.dense.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.8.layernorm_before.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.8.layernorm_before.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.8.layernorm_after.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.8.layernorm_after.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.9.attention.attention.query.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.9.attention.attention.query.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.9.attention.attention.key.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.9.attention.attention.key.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.9.attention.attention.value.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.9.attention.attention.value.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.9.attention.output.dense.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.9.attention.output.dense.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.9.intermediate.dense.weight": {
      "shape": [
        3072,
        768
      ],
      "dtype": "torch.float32",
      "size": 2359296,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.9.intermediate.dense.bias": {
      "shape": [
        3072
      ],
      "dtype": "torch.float32",
      "size": 3072,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.9.output.dense.weight": {
      "shape": [
        768,
        3072
      ],
      "dtype": "torch.float32",
      "size": 2359296,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.9.output.dense.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.9.layernorm_before.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.9.layernorm_before.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.9.layernorm_after.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.9.layernorm_after.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.10.attention.attention.query.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.10.attention.attention.query.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.10.attention.attention.key.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.10.attention.attention.key.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.10.attention.attention.value.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.10.attention.attention.value.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.10.attention.output.dense.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.10.attention.output.dense.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.10.intermediate.dense.weight": {
      "shape": [
        3072,
        768
      ],
      "dtype": "torch.float32",
      "size": 2359296,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.10.intermediate.dense.bias": {
      "shape": [
        3072
      ],
      "dtype": "torch.float32",
      "size": 3072,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.10.output.dense.weight": {
      "shape": [
        768,
        3072
      ],
      "dtype": "torch.float32",
      "size": 2359296,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.10.output.dense.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.10.layernorm_before.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.10.layernorm_before.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.10.layernorm_after.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.10.layernorm_after.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.11.attention.attention.query.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.11.attention.attention.query.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.11.attention.attention.key.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.11.attention.attention.key.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.11.attention.attention.value.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.11.attention.attention.value.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.11.attention.output.dense.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.11.attention.output.dense.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.11.intermediate.dense.weight": {
      "shape": [
        3072,
        768
      ],
      "dtype": "torch.float32",
      "size": 2359296,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.11.intermediate.dense.bias": {
      "shape": [
        3072
      ],
      "dtype": "torch.float32",
      "size": 3072,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.11.output.dense.weight": {
      "shape": [
        768,
        3072
      ],
      "dtype": "torch.float32",
      "size": 2359296,
      "ndim": 2
    },
    "image_tokenizer.model.encoder.layer.11.output.dense.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.11.layernorm_before.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.11.layernorm_before.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.11.layernorm_after.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.encoder.layer.11.layernorm_after.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.layernorm.weight": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.layernorm.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "image_tokenizer.model.pooler.dense.weight": {
      "shape": [
        768,
        768
      ],
      "dtype": "torch.float32",
      "size": 589824,
      "ndim": 2
    },
    "image_tokenizer.model.pooler.dense.bias": {
      "shape": [
        768
      ],
      "dtype": "torch.float32",
      "size": 768,
      "ndim": 1
    },
    "tokenizer.embeddings": {
      "shape": [
        3,
        1024,
        32,
        32
      ],
      "dtype": "torch.float32",
      "size": 3145728,
      "ndim": 4
    },
    "backbone.norm.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.norm.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.proj_in.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.proj_in.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.0.norm1.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.0.norm1.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.0.attn1.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.0.attn1.to_k.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.0.attn1.to_v.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.0.attn1.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.0.attn1.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.0.norm2.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.0.norm2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.0.attn2.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.0.attn2.to_k.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.0.attn2.to_v.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.0.attn2.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.0.attn2.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.0.norm3.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.0.norm3.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.0.ff.net.0.proj.weight": {
      "shape": [
        8192,
        1024
      ],
      "dtype": "torch.float32",
      "size": 8388608,
      "ndim": 2
    },
    "backbone.transformer_blocks.0.ff.net.0.proj.bias": {
      "shape": [
        8192
      ],
      "dtype": "torch.float32",
      "size": 8192,
      "ndim": 1
    },
    "backbone.transformer_blocks.0.ff.net.2.weight": {
      "shape": [
        1024,
        4096
      ],
      "dtype": "torch.float32",
      "size": 4194304,
      "ndim": 2
    },
    "backbone.transformer_blocks.0.ff.net.2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.1.norm1.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.1.norm1.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.1.attn1.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.1.attn1.to_k.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.1.attn1.to_v.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.1.attn1.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.1.attn1.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.1.norm2.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.1.norm2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.1.attn2.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.1.attn2.to_k.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.1.attn2.to_v.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.1.attn2.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.1.attn2.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.1.norm3.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.1.norm3.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.1.ff.net.0.proj.weight": {
      "shape": [
        8192,
        1024
      ],
      "dtype": "torch.float32",
      "size": 8388608,
      "ndim": 2
    },
    "backbone.transformer_blocks.1.ff.net.0.proj.bias": {
      "shape": [
        8192
      ],
      "dtype": "torch.float32",
      "size": 8192,
      "ndim": 1
    },
    "backbone.transformer_blocks.1.ff.net.2.weight": {
      "shape": [
        1024,
        4096
      ],
      "dtype": "torch.float32",
      "size": 4194304,
      "ndim": 2
    },
    "backbone.transformer_blocks.1.ff.net.2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.2.norm1.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.2.norm1.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.2.attn1.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.2.attn1.to_k.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.2.attn1.to_v.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.2.attn1.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.2.attn1.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.2.norm2.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.2.norm2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.2.attn2.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.2.attn2.to_k.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.2.attn2.to_v.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.2.attn2.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.2.attn2.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.2.norm3.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.2.norm3.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.2.ff.net.0.proj.weight": {
      "shape": [
        8192,
        1024
      ],
      "dtype": "torch.float32",
      "size": 8388608,
      "ndim": 2
    },
    "backbone.transformer_blocks.2.ff.net.0.proj.bias": {
      "shape": [
        8192
      ],
      "dtype": "torch.float32",
      "size": 8192,
      "ndim": 1
    },
    "backbone.transformer_blocks.2.ff.net.2.weight": {
      "shape": [
        1024,
        4096
      ],
      "dtype": "torch.float32",
      "size": 4194304,
      "ndim": 2
    },
    "backbone.transformer_blocks.2.ff.net.2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.3.norm1.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.3.norm1.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.3.attn1.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.3.attn1.to_k.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.3.attn1.to_v.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.3.attn1.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.3.attn1.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.3.norm2.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.3.norm2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.3.attn2.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.3.attn2.to_k.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.3.attn2.to_v.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.3.attn2.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.3.attn2.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.3.norm3.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.3.norm3.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.3.ff.net.0.proj.weight": {
      "shape": [
        8192,
        1024
      ],
      "dtype": "torch.float32",
      "size": 8388608,
      "ndim": 2
    },
    "backbone.transformer_blocks.3.ff.net.0.proj.bias": {
      "shape": [
        8192
      ],
      "dtype": "torch.float32",
      "size": 8192,
      "ndim": 1
    },
    "backbone.transformer_blocks.3.ff.net.2.weight": {
      "shape": [
        1024,
        4096
      ],
      "dtype": "torch.float32",
      "size": 4194304,
      "ndim": 2
    },
    "backbone.transformer_blocks.3.ff.net.2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.4.norm1.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.4.norm1.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.4.attn1.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.4.attn1.to_k.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.4.attn1.to_v.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.4.attn1.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.4.attn1.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.4.norm2.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.4.norm2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.4.attn2.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.4.attn2.to_k.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.4.attn2.to_v.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.4.attn2.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.4.attn2.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.4.norm3.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.4.norm3.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.4.ff.net.0.proj.weight": {
      "shape": [
        8192,
        1024
      ],
      "dtype": "torch.float32",
      "size": 8388608,
      "ndim": 2
    },
    "backbone.transformer_blocks.4.ff.net.0.proj.bias": {
      "shape": [
        8192
      ],
      "dtype": "torch.float32",
      "size": 8192,
      "ndim": 1
    },
    "backbone.transformer_blocks.4.ff.net.2.weight": {
      "shape": [
        1024,
        4096
      ],
      "dtype": "torch.float32",
      "size": 4194304,
      "ndim": 2
    },
    "backbone.transformer_blocks.4.ff.net.2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.5.norm1.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.5.norm1.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.5.attn1.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.5.attn1.to_k.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.5.attn1.to_v.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.5.attn1.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.5.attn1.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.5.norm2.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.5.norm2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.5.attn2.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.5.attn2.to_k.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.5.attn2.to_v.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.5.attn2.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.5.attn2.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.5.norm3.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.5.norm3.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.5.ff.net.0.proj.weight": {
      "shape": [
        8192,
        1024
      ],
      "dtype": "torch.float32",
      "size": 8388608,
      "ndim": 2
    },
    "backbone.transformer_blocks.5.ff.net.0.proj.bias": {
      "shape": [
        8192
      ],
      "dtype": "torch.float32",
      "size": 8192,
      "ndim": 1
    },
    "backbone.transformer_blocks.5.ff.net.2.weight": {
      "shape": [
        1024,
        4096
      ],
      "dtype": "torch.float32",
      "size": 4194304,
      "ndim": 2
    },
    "backbone.transformer_blocks.5.ff.net.2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.6.norm1.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.6.norm1.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.6.attn1.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.6.attn1.to_k.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.6.attn1.to_v.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.6.attn1.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.6.attn1.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.6.norm2.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.6.norm2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.6.attn2.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.6.attn2.to_k.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.6.attn2.to_v.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.6.attn2.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.6.attn2.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.6.norm3.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.6.norm3.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.6.ff.net.0.proj.weight": {
      "shape": [
        8192,
        1024
      ],
      "dtype": "torch.float32",
      "size": 8388608,
      "ndim": 2
    },
    "backbone.transformer_blocks.6.ff.net.0.proj.bias": {
      "shape": [
        8192
      ],
      "dtype": "torch.float32",
      "size": 8192,
      "ndim": 1
    },
    "backbone.transformer_blocks.6.ff.net.2.weight": {
      "shape": [
        1024,
        4096
      ],
      "dtype": "torch.float32",
      "size": 4194304,
      "ndim": 2
    },
    "backbone.transformer_blocks.6.ff.net.2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.7.norm1.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.7.norm1.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.7.attn1.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.7.attn1.to_k.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.7.attn1.to_v.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.7.attn1.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.7.attn1.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.7.norm2.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.7.norm2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.7.attn2.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.7.attn2.to_k.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.7.attn2.to_v.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.7.attn2.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.7.attn2.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.7.norm3.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.7.norm3.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.7.ff.net.0.proj.weight": {
      "shape": [
        8192,
        1024
      ],
      "dtype": "torch.float32",
      "size": 8388608,
      "ndim": 2
    },
    "backbone.transformer_blocks.7.ff.net.0.proj.bias": {
      "shape": [
        8192
      ],
      "dtype": "torch.float32",
      "size": 8192,
      "ndim": 1
    },
    "backbone.transformer_blocks.7.ff.net.2.weight": {
      "shape": [
        1024,
        4096
      ],
      "dtype": "torch.float32",
      "size": 4194304,
      "ndim": 2
    },
    "backbone.transformer_blocks.7.ff.net.2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.8.norm1.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.8.norm1.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.8.attn1.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.8.attn1.to_k.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.8.attn1.to_v.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.8.attn1.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.8.attn1.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.8.norm2.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.8.norm2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.8.attn2.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.8.attn2.to_k.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.8.attn2.to_v.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.8.attn2.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.8.attn2.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.8.norm3.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.8.norm3.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.8.ff.net.0.proj.weight": {
      "shape": [
        8192,
        1024
      ],
      "dtype": "torch.float32",
      "size": 8388608,
      "ndim": 2
    },
    "backbone.transformer_blocks.8.ff.net.0.proj.bias": {
      "shape": [
        8192
      ],
      "dtype": "torch.float32",
      "size": 8192,
      "ndim": 1
    },
    "backbone.transformer_blocks.8.ff.net.2.weight": {
      "shape": [
        1024,
        4096
      ],
      "dtype": "torch.float32",
      "size": 4194304,
      "ndim": 2
    },
    "backbone.transformer_blocks.8.ff.net.2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.9.norm1.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.9.norm1.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.9.attn1.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.9.attn1.to_k.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.9.attn1.to_v.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.9.attn1.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.9.attn1.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.9.norm2.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.9.norm2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.9.attn2.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.9.attn2.to_k.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.9.attn2.to_v.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.9.attn2.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.9.attn2.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.9.norm3.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.9.norm3.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.9.ff.net.0.proj.weight": {
      "shape": [
        8192,
        1024
      ],
      "dtype": "torch.float32",
      "size": 8388608,
      "ndim": 2
    },
    "backbone.transformer_blocks.9.ff.net.0.proj.bias": {
      "shape": [
        8192
      ],
      "dtype": "torch.float32",
      "size": 8192,
      "ndim": 1
    },
    "backbone.transformer_blocks.9.ff.net.2.weight": {
      "shape": [
        1024,
        4096
      ],
      "dtype": "torch.float32",
      "size": 4194304,
      "ndim": 2
    },
    "backbone.transformer_blocks.9.ff.net.2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.10.norm1.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.10.norm1.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.10.attn1.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.10.attn1.to_k.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.10.attn1.to_v.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.10.attn1.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.10.attn1.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.10.norm2.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.10.norm2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.10.attn2.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.10.attn2.to_k.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.10.attn2.to_v.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.10.attn2.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.10.attn2.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.10.norm3.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.10.norm3.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.10.ff.net.0.proj.weight": {
      "shape": [
        8192,
        1024
      ],
      "dtype": "torch.float32",
      "size": 8388608,
      "ndim": 2
    },
    "backbone.transformer_blocks.10.ff.net.0.proj.bias": {
      "shape": [
        8192
      ],
      "dtype": "torch.float32",
      "size": 8192,
      "ndim": 1
    },
    "backbone.transformer_blocks.10.ff.net.2.weight": {
      "shape": [
        1024,
        4096
      ],
      "dtype": "torch.float32",
      "size": 4194304,
      "ndim": 2
    },
    "backbone.transformer_blocks.10.ff.net.2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.11.norm1.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.11.norm1.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.11.attn1.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.11.attn1.to_k.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.11.attn1.to_v.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.11.attn1.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.11.attn1.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.11.norm2.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.11.norm2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.11.attn2.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.11.attn2.to_k.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.11.attn2.to_v.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.11.attn2.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.11.attn2.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.11.norm3.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.11.norm3.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.11.ff.net.0.proj.weight": {
      "shape": [
        8192,
        1024
      ],
      "dtype": "torch.float32",
      "size": 8388608,
      "ndim": 2
    },
    "backbone.transformer_blocks.11.ff.net.0.proj.bias": {
      "shape": [
        8192
      ],
      "dtype": "torch.float32",
      "size": 8192,
      "ndim": 1
    },
    "backbone.transformer_blocks.11.ff.net.2.weight": {
      "shape": [
        1024,
        4096
      ],
      "dtype": "torch.float32",
      "size": 4194304,
      "ndim": 2
    },
    "backbone.transformer_blocks.11.ff.net.2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.12.norm1.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.12.norm1.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.12.attn1.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.12.attn1.to_k.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.12.attn1.to_v.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.12.attn1.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.12.attn1.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.12.norm2.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.12.norm2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.12.attn2.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.12.attn2.to_k.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.12.attn2.to_v.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.12.attn2.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.12.attn2.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.12.norm3.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.12.norm3.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.12.ff.net.0.proj.weight": {
      "shape": [
        8192,
        1024
      ],
      "dtype": "torch.float32",
      "size": 8388608,
      "ndim": 2
    },
    "backbone.transformer_blocks.12.ff.net.0.proj.bias": {
      "shape": [
        8192
      ],
      "dtype": "torch.float32",
      "size": 8192,
      "ndim": 1
    },
    "backbone.transformer_blocks.12.ff.net.2.weight": {
      "shape": [
        1024,
        4096
      ],
      "dtype": "torch.float32",
      "size": 4194304,
      "ndim": 2
    },
    "backbone.transformer_blocks.12.ff.net.2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.13.norm1.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.13.norm1.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.13.attn1.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.13.attn1.to_k.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.13.attn1.to_v.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.13.attn1.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.13.attn1.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.13.norm2.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.13.norm2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.13.attn2.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.13.attn2.to_k.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.13.attn2.to_v.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.13.attn2.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.13.attn2.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.13.norm3.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.13.norm3.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.13.ff.net.0.proj.weight": {
      "shape": [
        8192,
        1024
      ],
      "dtype": "torch.float32",
      "size": 8388608,
      "ndim": 2
    },
    "backbone.transformer_blocks.13.ff.net.0.proj.bias": {
      "shape": [
        8192
      ],
      "dtype": "torch.float32",
      "size": 8192,
      "ndim": 1
    },
    "backbone.transformer_blocks.13.ff.net.2.weight": {
      "shape": [
        1024,
        4096
      ],
      "dtype": "torch.float32",
      "size": 4194304,
      "ndim": 2
    },
    "backbone.transformer_blocks.13.ff.net.2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.14.norm1.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.14.norm1.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.14.attn1.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.14.attn1.to_k.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.14.attn1.to_v.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.14.attn1.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.14.attn1.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.14.norm2.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.14.norm2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.14.attn2.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.14.attn2.to_k.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.14.attn2.to_v.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.14.attn2.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.14.attn2.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.14.norm3.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.14.norm3.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.14.ff.net.0.proj.weight": {
      "shape": [
        8192,
        1024
      ],
      "dtype": "torch.float32",
      "size": 8388608,
      "ndim": 2
    },
    "backbone.transformer_blocks.14.ff.net.0.proj.bias": {
      "shape": [
        8192
      ],
      "dtype": "torch.float32",
      "size": 8192,
      "ndim": 1
    },
    "backbone.transformer_blocks.14.ff.net.2.weight": {
      "shape": [
        1024,
        4096
      ],
      "dtype": "torch.float32",
      "size": 4194304,
      "ndim": 2
    },
    "backbone.transformer_blocks.14.ff.net.2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.15.norm1.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.15.norm1.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.15.attn1.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.15.attn1.to_k.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.15.attn1.to_v.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.15.attn1.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.15.attn1.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.15.norm2.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.15.norm2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.15.attn2.to_q.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.15.attn2.to_k.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.15.attn2.to_v.weight": {
      "shape": [
        1024,
        768
      ],
      "dtype": "torch.float32",
      "size": 786432,
      "ndim": 2
    },
    "backbone.transformer_blocks.15.attn2.to_out.0.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.transformer_blocks.15.attn2.to_out.0.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.15.norm3.weight": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.15.norm3.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.transformer_blocks.15.ff.net.0.proj.weight": {
      "shape": [
        8192,
        1024
      ],
      "dtype": "torch.float32",
      "size": 8388608,
      "ndim": 2
    },
    "backbone.transformer_blocks.15.ff.net.0.proj.bias": {
      "shape": [
        8192
      ],
      "dtype": "torch.float32",
      "size": 8192,
      "ndim": 1
    },
    "backbone.transformer_blocks.15.ff.net.2.weight": {
      "shape": [
        1024,
        4096
      ],
      "dtype": "torch.float32",
      "size": 4194304,
      "ndim": 2
    },
    "backbone.transformer_blocks.15.ff.net.2.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "backbone.proj_out.weight": {
      "shape": [
        1024,
        1024
      ],
      "dtype": "torch.float32",
      "size": 1048576,
      "ndim": 2
    },
    "backbone.proj_out.bias": {
      "shape": [
        1024
      ],
      "dtype": "torch.float32",
      "size": 1024,
      "ndim": 1
    },
    "post_processor.upsample.weight": {
      "shape": [
        1024,
        40,
        2,
        2
      ],
      "dtype": "torch.float32",
      "size": 163840,
      "ndim": 4
    },
    "post_processor.upsample.bias": {
      "shape": [
        40
      ],
      "dtype": "torch.float32",
      "size": 40,
      "ndim": 1
    },
    "decoder.layers.0.weight": {
      "shape": [
        64,
        120
      ],
      "dtype": "torch.float32",
      "size": 7680,
      "ndim": 2
    },
    "decoder.layers.0.bias": {
      "shape": [
        64
      ],
      "dtype": "torch.float32",
      "size": 64,
      "ndim": 1
    },
    "decoder.layers.2.weight": {
      "shape": [
        64,
        64
      ],
      "dtype": "torch.float32",
      "size": 4096,
      "ndim": 2
    },
    "decoder.layers.2.bias": {
      "shape": [
        64
      ],
      "dtype": "torch.float32",
      "size": 64,
      "ndim": 1
    },
    "decoder.layers.4.weight": {
      "shape": [
        64,
        64
      ],
      "dtype": "torch.float32",
      "size": 4096,
      "ndim": 2
    },
    "decoder.layers.4.bias": {
      "shape": [
        64
      ],
      "dtype": "torch.float32",
      "size": 64,
      "ndim": 1
    },
    "decoder.layers.6.weight": {
      "shape": [
        64,
        64
      ],
      "dtype": "torch.float32",
      "size": 4096,
      "ndim": 2
    },
    "decoder.layers.6.bias": {
      "shape": [
        64
      ],
      "dtype": "torch.float32",
      "size": 64,
      "ndim": 1
    },
    "decoder.layers.8.weight": {
      "shape": [
        64,
        64
      ],
      "dtype": "torch.float32",
      "size": 4096,
      "ndim": 2
    },
    "decoder.layers.8.bias": {
      "shape": [
        64
      ],
      "dtype": "torch.float32",
      "size": 64,
      "ndim": 1
    },
    "decoder.layers.10.weight": {
      "shape": [
        64,
        64
      ],
      "dtype": "torch.float32",
      "size": 4096,
      "ndim": 2
    },
    "decoder.layers.10.bias": {
      "shape": [
        64
      ],
      "dtype": "torch.float32",
      "size": 64,
      "ndim": 1
    },
    "decoder.layers.12.weight": {
      "shape": [
        64,
        64
      ],
      "dtype": "torch.float32",
      "size": 4096,
      "ndim": 2
    },
    "decoder.layers.12.bias": {
      "shape": [
        64
      ],
      "dtype": "torch.float32",
      "size": 64,
      "ndim": 1
    },
    "decoder.layers.14.weight": {
      "shape": [
        64,
        64
      ],
      "dtype": "torch.float32",
      "size": 4096,
      "ndim": 2
    },
    "decoder.layers.14.bias": {
      "shape": [
        64
      ],
      "dtype": "torch.float32",
      "size": 64,
      "ndim": 1
    },
    "decoder.layers.16.weight": {
      "shape": [
        64,
        64
      ],
      "dtype": "torch.float32",
      "size": 4096,
      "ndim": 2
    },
    "decoder.layers.16.bias": {
      "shape": [
        64
      ],
      "dtype": "torch.float32",
      "size": 64,
      "ndim": 1
    },
    "decoder.layers.18.weight": {
      "shape": [
        4,
        64
      ],
      "dtype": "torch.float32",
      "size": 256,
      "ndim": 2
    },
    "decoder.layers.18.bias": {
      "shape": [
        4
      ],
      "dtype": "torch.float32",
      "size": 4,
      "ndim": 1
    }
  },
  "structure": {
    "total_parameters": 549,
    "parameter_groups": {
      "image_tokenizer": [
        "image_tokenizer.model.embeddings.cls_token",
        "image_tokenizer.model.embeddings.position_embeddings",
        "image_tokenizer.model.embeddings.patch_embeddings.projection.weight",
        "image_tokenizer.model.embeddings.patch_embeddings.projection.bias",
        "image_tokenizer.model.encoder.layer.0.attention.attention.query.weight",
        "image_tokenizer.model.encoder.layer.0.attention.attention.query.bias",
        "image_tokenizer.model.encoder.layer.0.attention.attention.key.weight",
        "image_tokenizer.model.encoder.layer.0.attention.attention.key.bias",
        "image_tokenizer.model.encoder.layer.0.attention.attention.value.weight",
        "image_tokenizer.model.encoder.layer.0.attention.attention.value.bias",
        "image_tokenizer.model.encoder.layer.0.attention.output.dense.weight",
        "image_tokenizer.model.encoder.layer.0.attention.output.dense.bias",
        "image_tokenizer.model.encoder.layer.0.intermediate.dense.weight",
        "image_tokenizer.model.encoder.layer.0.intermediate.dense.bias",
        "image_tokenizer.model.encoder.layer.0.output.dense.weight",
        "image_tokenizer.model.encoder.layer.0.output.dense.bias",
        "image_tokenizer.model.encoder.layer.0.layernorm_before.weight",
        "image_tokenizer.model.encoder.layer.0.layernorm_before.bias",
        "image_tokenizer.model.encoder.layer.0.layernorm_after.weight",
        "image_tokenizer.model.encoder.layer.0.layernorm_after.bias",
        "image_tokenizer.model.encoder.layer.1.attention.attention.query.weight",
        "image_tokenizer.model.encoder.layer.1.attention.attention.query.bias",
        "image_tokenizer.model.encoder.layer.1.attention.attention.key.weight",
        "image_tokenizer.model.encoder.layer.1.attention.attention.key.bias",
        "image_tokenizer.model.encoder.layer.1.attention.attention.value.weight",
        "image_tokenizer.model.encoder.layer.1.attention.attention.value.bias",
        "image_tokenizer.model.encoder.layer.1.attention.output.dense.weight",
        "image_tokenizer.model.encoder.layer.1.attention.output.dense.bias",
        "image_tokenizer.model.encoder.layer.1.intermediate.dense.weight",
        "image_tokenizer.model.encoder.layer.1.intermediate.dense.bias",
        "image_tokenizer.model.encoder.layer.1.output.dense.weight",
        "image_tokenizer.model.encoder.layer.1.output.dense.bias",
        "image_tokenizer.model.encoder.layer.1.layernorm_before.weight",
        "image_tokenizer.model.encoder.layer.1.layernorm_before.bias",
        "image_tokenizer.model.encoder.layer.1.layernorm_after.weight",
        "image_tokenizer.model.encoder.layer.1.layernorm_after.bias",
        "image_tokenizer.model.encoder.layer.2.attention.attention.query.weight",
        "image_tokenizer.model.encoder.layer.2.attention.attention.query.bias",
        "image_tokenizer.model.encoder.layer.2.attention.attention.key.weight",
        "image_tokenizer.model.encoder.layer.2.attention.attention.key.bias",
        "image_tokenizer.model.encoder.layer.2.attention.attention.value.weight",
        "image_tokenizer.model.encoder.layer.2.attention.attention.value.bias",
        "image_tokenizer.model.encoder.layer.2.attention.output.dense.weight",
        "image_tokenizer.model.encoder.layer.2.attention.output.dense.bias",
        "image_tokenizer.model.encoder.layer.2.intermediate.dense.weight",
        "image_tokenizer.model.encoder.layer.2.intermediate.dense.bias",
        "image_tokenizer.model.encoder.layer.2.output.dense.weight",
        "image_tokenizer.model.encoder.layer.2.output.dense.bias",
        "image_tokenizer.model.encoder.layer.2.layernorm_before.weight",
        "image_tokenizer.model.encoder.layer.2.layernorm_before.bias",
        "image_tokenizer.model.encoder.layer.2.layernorm_after.weight",
        "image_tokenizer.model.encoder.layer.2.layernorm_after.bias",
        "image_tokenizer.model.encoder.layer.3.attention.attention.query.weight",
        "image_tokenizer.model.encoder.layer.3.attention.attention.query.bias",
        "image_tokenizer.model.encoder.layer.3.attention.attention.key.weight",
        "image_tokenizer.model.encoder.layer.3.attention.attention.key.bias",
        "image_tokenizer.model.encoder.layer.3.attention.attention.value.weight",
        "image_tokenizer.model.encoder.layer.3.attention.attention.value.bias",
        "image_tokenizer.model.encoder.layer.3.attention.output.dense.weight",
        "image_tokenizer.model.encoder.layer.3.attention.output.dense.bias",
        "image_tokenizer.model.encoder.layer.3.intermediate.dense.weight",
        "image_tokenizer.model.encoder.layer.3.intermediate.dense.bias",
        "image_tokenizer.model.encoder.layer.3.output.dense.weight",
        "image_tokenizer.model.encoder.layer.3.output.dense.bias",
        "image_tokenizer.model.encoder.layer.3.layernorm_before.weight",
        "image_tokenizer.model.encoder.layer.3.layernorm_before.bias",
        "image_tokenizer.model.encoder.layer.3.layernorm_after.weight",
        "image_tokenizer.model.encoder.layer.3.layernorm_after.bias",
        "image_tokenizer.model.encoder.layer.4.attention.attention.query.weight",
        "image_tokenizer.model.encoder.layer.4.attention.attention.query.bias",
        "image_tokenizer.model.encoder.layer.4.attention.attention.key.weight",
        "image_tokenizer.model.encoder.layer.4.attention.attention.key.bias",
        "image_tokenizer.model.encoder.layer.4.attention.attention.value.weight",
        "image_tokenizer.model.encoder.layer.4.attention.attention.value.bias",
        "image_tokenizer.model.encoder.layer.4.attention.output.dense.weight",
        "image_tokenizer.model.encoder.layer.4.attention.output.dense.bias",
        "image_tokenizer.model.encoder.layer.4.intermediate.dense.weight",
        "image_tokenizer.model.encoder.layer.4.intermediate.dense.bias",
        "image_tokenizer.model.encoder.layer.4.output.dense.weight",
        "image_tokenizer.model.encoder.layer.4.output.dense.bias",
        "image_tokenizer.model.encoder.layer.4.layernorm_before.weight",
        "image_tokenizer.model.encoder.layer.4.layernorm_before.bias",
        "image_tokenizer.model.encoder.layer.4.layernorm_after.weight",
        "image_tokenizer.model.encoder.layer.4.layernorm_after.bias",
        "image_tokenizer.model.encoder.layer.5.attention.attention.query.weight",
        "image_tokenizer.model.encoder.layer.5.attention.attention.query.bias",
        "image_tokenizer.model.encoder.layer.5.attention.attention.key.weight",
        "image_tokenizer.model.encoder.layer.5.attention.attention.key.bias",
        "image_tokenizer.model.encoder.layer.5.attention.attention.value.weight",
        "image_tokenizer.model.encoder.layer.5.attention.attention.value.bias",
        "image_tokenizer.model.encoder.layer.5.attention.output.dense.weight",
        "image_tokenizer.model.encoder.layer.5.attention.output.dense.bias",
        "image_tokenizer.model.encoder.layer.5.intermediate.dense.weight",
        "image_tokenizer.model.encoder.layer.5.intermediate.dense.bias",
        "image_tokenizer.model.encoder.layer.5.output.dense.weight",
        "image_tokenizer.model.encoder.layer.5.output.dense.bias",
        "image_tokenizer.model.encoder.layer.5.layernorm_before.weight",
        "image_tokenizer.model.encoder.layer.5.layernorm_before.bias",
        "image_tokenizer.model.encoder.layer.5.layernorm_after.weight",
        "image_tokenizer.model.encoder.layer.5.layernorm_after.bias",
        "image_tokenizer.model.encoder.layer.6.attention.attention.query.weight",
        "image_tokenizer.model.encoder.layer.6.attention.attention.query.bias",
        "image_tokenizer.model.encoder.layer.6.attention.attention.key.weight",
        "image_tokenizer.model.encoder.layer.6.attention.attention.key.bias",
        "image_tokenizer.model.encoder.layer.6.attention.attention.value.weight",
        "image_tokenizer.model.encoder.layer.6.attention.attention.value.bias",
        "image_tokenizer.model.encoder.layer.6.attention.output.dense.weight",
        "image_tokenizer.model.encoder.layer.6.attention.output.dense.bias",
        "image_tokenizer.model.encoder.layer.6.intermediate.dense.weight",
        "image_tokenizer.model.encoder.layer.6.intermediate.dense.bias",
        "image_tokenizer.model.encoder.layer.6.output.dense.weight",
        "image_tokenizer.model.encoder.layer.6.output.dense.bias",
        "image_tokenizer.model.encoder.layer.6.layernorm_before.weight",
        "image_tokenizer.model.encoder.layer.6.layernorm_before.bias",
        "image_tokenizer.model.encoder.layer.6.layernorm_after.weight",
        "image_tokenizer.model.encoder.layer.6.layernorm_after.bias",
        "image_tokenizer.model.encoder.layer.7.attention.attention.query.weight",
        "image_tokenizer.model.encoder.layer.7.attention.attention.query.bias",
        "image_tokenizer.model.encoder.layer.7.attention.attention.key.weight",
        "image_tokenizer.model.encoder.layer.7.attention.attention.key.bias",
        "image_tokenizer.model.encoder.layer.7.attention.attention.value.weight",
        "image_tokenizer.model.encoder.layer.7.attention.attention.value.bias",
        "image_tokenizer.model.encoder.layer.7.attention.output.dense.weight",
        "image_tokenizer.model.encoder.layer.7.attention.output.dense.bias",
        "image_tokenizer.model.encoder.layer.7.intermediate.dense.weight",
        "image_tokenizer.model.encoder.layer.7.intermediate.dense.bias",
        "image_tokenizer.model.encoder.layer.7.output.dense.weight",
        "image_tokenizer.model.encoder.layer.7.output.dense.bias",
        "image_tokenizer.model.encoder.layer.7.layernorm_before.weight",
        "image_tokenizer.model.encoder.layer.7.layernorm_before.bias",
        "image_tokenizer.model.encoder.layer.7.layernorm_after.weight",
        "image_tokenizer.model.encoder.layer.7.layernorm_after.bias",
        "image_tokenizer.model.encoder.layer.8.attention.attention.query.weight",
        "image_tokenizer.model.encoder.layer.8.attention.attention.query.bias",
        "image_tokenizer.model.encoder.layer.8.attention.attention.key.weight",
        "image_tokenizer.model.encoder.layer.8.attention.attention.key.bias",
        "image_tokenizer.model.encoder.layer.8.attention.attention.value.weight",
        "image_tokenizer.model.encoder.layer.8.attention.attention.value.bias",
        "image_tokenizer.model.encoder.layer.8.attention.output.dense.weight",
        "image_tokenizer.model.encoder.layer.8.attention.output.dense.bias",
        "image_tokenizer.model.encoder.layer.8.intermediate.dense.weight",
        "image_tokenizer.model.encoder.layer.8.intermediate.dense.bias",
        "image_tokenizer.model.encoder.layer.8.output.dense.weight",
        "image_tokenizer.model.encoder.layer.8.output.dense.bias",
        "image_tokenizer.model.encoder.layer.8.layernorm_before.weight",
        "image_tokenizer.model.encoder.layer.8.layernorm_before.bias",
        "image_tokenizer.model.encoder.layer.8.layernorm_after.weight",
        "image_tokenizer.model.encoder.layer.8.layernorm_after.bias",
        "image_tokenizer.model.encoder.layer.9.attention.attention.query.weight",
        "image_tokenizer.model.encoder.layer.9.attention.attention.query.bias",
        "image_tokenizer.model.encoder.layer.9.attention.attention.key.weight",
        "image_tokenizer.model.encoder.layer.9.attention.attention.key.bias",
        "image_tokenizer.model.encoder.layer.9.attention.attention.value.weight",
        "image_tokenizer.model.encoder.layer.9.attention.attention.value.bias",
        "image_tokenizer.model.encoder.layer.9.attention.output.dense.weight",
        "image_tokenizer.model.encoder.layer.9.attention.output.dense.bias",
        "image_tokenizer.model.encoder.layer.9.intermediate.dense.weight",
        "image_tokenizer.model.encoder.layer.9.intermediate.dense.bias",
        "image_tokenizer.model.encoder.layer.9.output.dense.weight",
        "image_tokenizer.model.encoder.layer.9.output.dense.bias",
        "image_tokenizer.model.encoder.layer.9.layernorm_before.weight",
        "image_tokenizer.model.encoder.layer.9.layernorm_before.bias",
        "image_tokenizer.model.encoder.layer.9.layernorm_after.weight",
        "image_tokenizer.model.encoder.layer.9.layernorm_after.bias",
        "image_tokenizer.model.encoder.layer.10.attention.attention.query.weight",
        "image_tokenizer.model.encoder.layer.10.attention.attention.query.bias",
        "image_tokenizer.model.encoder.layer.10.attention.attention.key.weight",
        "image_tokenizer.model.encoder.layer.10.attention.attention.key.bias",
        "image_tokenizer.model.encoder.layer.10.attention.attention.value.weight",
        "image_tokenizer.model.encoder.layer.10.attention.attention.value.bias",
        "image_tokenizer.model.encoder.layer.10.attention.output.dense.weight",
        "image_tokenizer.model.encoder.layer.10.attention.output.dense.bias",
        "image_tokenizer.model.encoder.layer.10.intermediate.dense.weight",
        "image_tokenizer.model.encoder.layer.10.intermediate.dense.bias",
        "image_tokenizer.model.encoder.layer.10.output.dense.weight",
        "image_tokenizer.model.encoder.layer.10.output.dense.bias",
        "image_tokenizer.model.encoder.layer.10.layernorm_before.weight",
        "image_tokenizer.model.encoder.layer.10.layernorm_before.bias",
        "image_tokenizer.model.encoder.layer.10.layernorm_after.weight",
        "image_tokenizer.model.encoder.layer.10.layernorm_after.bias",
        "image_tokenizer.model.encoder.layer.11.attention.attention.query.weight",
        "image_tokenizer.model.encoder.layer.11.attention.attention.query.bias",
        "image_tokenizer.model.encoder.layer.11.attention.attention.key.weight",
        "image_tokenizer.model.encoder.layer.11.attention.attention.key.bias",
        "image_tokenizer.model.encoder.layer.11.attention.attention.value.weight",
        "image_tokenizer.model.encoder.layer.11.attention.attention.value.bias",
        "image_tokenizer.model.encoder.layer.11.attention.output.dense.weight",
        "image_tokenizer.model.encoder.layer.11.attention.output.dense.bias",
        "image_tokenizer.model.encoder.layer.11.intermediate.dense.weight",
        "image_tokenizer.model.encoder.layer.11.intermediate.dense.bias",
        "image_tokenizer.model.encoder.layer.11.output.dense.weight",
        "image_tokenizer.model.encoder.layer.11.output.dense.bias",
        "image_tokenizer.model.encoder.layer.11.layernorm_before.weight",
        "image_tokenizer.model.encoder.layer.11.layernorm_before.bias",
        "image_tokenizer.model.encoder.layer.11.layernorm_after.weight",
        "image_tokenizer.model.encoder.layer.11.layernorm_after.bias",
        "image_tokenizer.model.layernorm.weight",
        "image_tokenizer.model.layernorm.bias",
        "image_tokenizer.model.pooler.dense.weight",
        "image_tokenizer.model.pooler.dense.bias"
      ],
      "tokenizer": [
        "tokenizer.embeddings"
      ],
      "backbone": [
        "backbone.norm.weight",
        "backbone.norm.bias",
        "backbone.proj_in.weight",
        "backbone.proj_in.bias",
        "backbone.transformer_blocks.0.norm1.weight",
        "backbone.transformer_blocks.0.norm1.bias",
        "backbone.transformer_blocks.0.attn1.to_q.weight",
        "backbone.transformer_blocks.0.attn1.to_k.weight",
        "backbone.transformer_blocks.0.attn1.to_v.weight",
        "backbone.transformer_blocks.0.attn1.to_out.0.weight",
        "backbone.transformer_blocks.0.attn1.to_out.0.bias",
        "backbone.transformer_blocks.0.norm2.weight",
        "backbone.transformer_blocks.0.norm2.bias",
        "backbone.transformer_blocks.0.attn2.to_q.weight",
        "backbone.transformer_blocks.0.attn2.to_k.weight",
        "backbone.transformer_blocks.0.attn2.to_v.weight",
        "backbone.transformer_blocks.0.attn2.to_out.0.weight",
        "backbone.transformer_blocks.0.attn2.to_out.0.bias",
        "backbone.transformer_blocks.0.norm3.weight",
        "backbone.transformer_blocks.0.norm3.bias",
        "backbone.transformer_blocks.0.ff.net.0.proj.weight",
        "backbone.transformer_blocks.0.ff.net.0.proj.bias",
        "backbone.transformer_blocks.0.ff.net.2.weight",
        "backbone.transformer_blocks.0.ff.net.2.bias",
        "backbone.transformer_blocks.1.norm1.weight",
        "backbone.transformer_blocks.1.norm1.bias",
        "backbone.transformer_blocks.1.attn1.to_q.weight",
        "backbone.transformer_blocks.1.attn1.to_k.weight",
        "backbone.transformer_blocks.1.attn1.to_v.weight",
        "backbone.transformer_blocks.1.attn1.to_out.0.weight",
        "backbone.transformer_blocks.1.attn1.to_out.0.bias",
        "backbone.transformer_blocks.1.norm2.weight",
        "backbone.transformer_blocks.1.norm2.bias",
        "backbone.transformer_blocks.1.attn2.to_q.weight",
        "backbone.transformer_blocks.1.attn2.to_k.weight",
        "backbone.transformer_blocks.1.attn2.to_v.weight",
        "backbone.transformer_blocks.1.attn2.to_out.0.weight",
        "backbone.transformer_blocks.1.attn2.to_out.0.bias",
        "backbone.transformer_blocks.1.norm3.weight",
        "backbone.transformer_blocks.1.norm3.bias",
        "backbone.transformer_blocks.1.ff.net.0.proj.weight",
        "backbone.transformer_blocks.1.ff.net.0.proj.bias",
        "backbone.transformer_blocks.1.ff.net.2.weight",
        "backbone.transformer_blocks.1.ff.net.2.bias",
        "backbone.transformer_blocks.2.norm1.weight",
        "backbone.transformer_blocks.2.norm1.bias",
        "backbone.transformer_blocks.2.attn1.to_q.weight",
        "backbone.transformer_blocks.2.attn1.to_k.weight",
        "backbone.transformer_blocks.2.attn1.to_v.weight",
        "backbone.transformer_blocks.2.attn1.to_out.0.weight",
        "backbone.transformer_blocks.2.attn1.to_out.0.bias",
        "backbone.transformer_blocks.2.norm2.weight",
        "backbone.transformer_blocks.2.norm2.bias",
        "backbone.transformer_blocks.2.attn2.to_q.weight",
        "backbone.transformer_blocks.2.attn2.to_k.weight",
        "backbone.transformer_blocks.2.attn2.to_v.weight",
        "backbone.transformer_blocks.2.attn2.to_out.0.weight",
        "backbone.transformer_blocks.2.attn2.to_out.0.bias",
        "backbone.transformer_blocks.2.norm3.weight",
        "backbone.transformer_blocks.2.norm3.bias",
        "backbone.transformer_blocks.2.ff.net.0.proj.weight",
        "backbone.transformer_blocks.2.ff.net.0.proj.bias",
        "backbone.transformer_blocks.2.ff.net.2.weight",
        "backbone.transformer_blocks.2.ff.net.2.bias",
        "backbone.transformer_blocks.3.norm1.weight",
        "backbone.transformer_blocks.3.norm1.bias",
        "backbone.transformer_blocks.3.attn1.to_q.weight",
        "backbone.transformer_blocks.3.attn1.to_k.weight",
        "backbone.transformer_blocks.3.attn1.to_v.weight",
        "backbone.transformer_blocks.3.attn1.to_out.0.weight",
        "backbone.transformer_blocks.3.attn1.to_out.0.bias",
        "backbone.transformer_blocks.3.norm2.weight",
        "backbone.transformer_blocks.3.norm2.bias",
        "backbone.transformer_blocks.3.attn2.to_q.weight",
        "backbone.transformer_blocks.3.attn2.to_k.weight",
        "backbone.transformer_blocks.3.attn2.to_v.weight",
        "backbone.transformer_blocks.3.attn2.to_out.0.weight",
        "backbone.transformer_blocks.3.attn2.to_out.0.bias",
        "backbone.transformer_blocks.3.norm3.weight",
        "backbone.transformer_blocks.3.norm3.bias",
        "backbone.transformer_blocks.3.ff.net.0.proj.weight",
        "backbone.transformer_blocks.3.ff.net.0.proj.bias",
        "backbone.transformer_blocks.3.ff.net.2.weight",
        "backbone.transformer_blocks.3.ff.net.2.bias",
        "backbone.transformer_blocks.4.norm1.weight",
        "backbone.transformer_blocks.4.norm1.bias",
        "backbone.transformer_blocks.4.attn1.to_q.weight",
        "backbone.transformer_blocks.4.attn1.to_k.weight",
        "backbone.transformer_blocks.4.attn1.to_v.weight",
        "backbone.transformer_blocks.4.attn1.to_out.0.weight",
        "backbone.transformer_blocks.4.attn1.to_out.0.bias",
        "backbone.transformer_blocks.4.norm2.weight",
        "backbone.transformer_blocks.4.norm2.bias",
        "backbone.transformer_blocks.4.attn2.to_q.weight",
        "backbone.transformer_blocks.4.attn2.to_k.weight",
        "backbone.transformer_blocks.4.attn2.to_v.weight",
        "backbone.transformer_blocks.4.attn2.to_out.0.weight",
        "backbone.transformer_blocks.4.attn2.to_out.0.bias",
        "backbone.transformer_blocks.4.norm3.weight",
        "backbone.transformer_blocks.4.norm3.bias",
        "backbone.transformer_blocks.4.ff.net.0.proj.weight",
        "backbone.transformer_blocks.4.ff.net.0.proj.bias",
        "backbone.transformer_blocks.4.ff.net.2.weight",
        "backbone.transformer_blocks.4.ff.net.2.bias",
        "backbone.transformer_blocks.5.norm1.weight",
        "backbone.transformer_blocks.5.norm1.bias",
        "backbone.transformer_blocks.5.attn1.to_q.weight",
        "backbone.transformer_blocks.5.attn1.to_k.weight",
        "backbone.transformer_blocks.5.attn1.to_v.weight",
        "backbone.transformer_blocks.5.attn1.to_out.0.weight",
        "backbone.transformer_blocks.5.attn1.to_out.0.bias",
        "backbone.transformer_blocks.5.norm2.weight",
        "backbone.transformer_blocks.5.norm2.bias",
        "backbone.transformer_blocks.5.attn2.to_q.weight",
        "backbone.transformer_blocks.5.attn2.to_k.weight",
        "backbone.transformer_blocks.5.attn2.to_v.weight",
        "backbone.transformer_blocks.5.attn2.to_out.0.weight",
        "backbone.transformer_blocks.5.attn2.to_out.0.bias",
        "backbone.transformer_blocks.5.norm3.weight",
        "backbone.transformer_blocks.5.norm3.bias",
        "backbone.transformer_blocks.5.ff.net.0.proj.weight",
        "backbone.transformer_blocks.5.ff.net.0.proj.bias",
        "backbone.transformer_blocks.5.ff.net.2.weight",
        "backbone.transformer_blocks.5.ff.net.2.bias",
        "backbone.transformer_blocks.6.norm1.weight",
        "backbone.transformer_blocks.6.norm1.bias",
        "backbone.transformer_blocks.6.attn1.to_q.weight",
        "backbone.transformer_blocks.6.attn1.to_k.weight",
        "backbone.transformer_blocks.6.attn1.to_v.weight",
        "backbone.transformer_blocks.6.attn1.to_out.0.weight",
        "backbone.transformer_blocks.6.attn1.to_out.0.bias",
        "backbone.transformer_blocks.6.norm2.weight",
        "backbone.transformer_blocks.6.norm2.bias",
        "backbone.transformer_blocks.6.attn2.to_q.weight",
        "backbone.transformer_blocks.6.attn2.to_k.weight",
        "backbone.transformer_blocks.6.attn2.to_v.weight",
        "backbone.transformer_blocks.6.attn2.to_out.0.weight",
        "backbone.transformer_blocks.6.attn2.to_out.0.bias",
        "backbone.transformer_blocks.6.norm3.weight",
        "backbone.transformer_blocks.6.norm3.bias",
        "backbone.transformer_blocks.6.ff.net.0.proj.weight",
        "backbone.transformer_blocks.6.ff.net.0.proj.bias",
        "backbone.transformer_blocks.6.ff.net.2.weight",
        "backbone.transformer_blocks.6.ff.net.2.bias",
        "backbone.transformer_blocks.7.norm1.weight",
        "backbone.transformer_blocks.7.norm1.bias",
        "backbone.transformer_blocks.7.attn1.to_q.weight",
        "backbone.transformer_blocks.7.attn1.to_k.weight",
        "backbone.transformer_blocks.7.attn1.to_v.weight",
        "backbone.transformer_blocks.7.attn1.to_out.0.weight",
        "backbone.transformer_blocks.7.attn1.to_out.0.bias",
        "backbone.transformer_blocks.7.norm2.weight",
        "backbone.transformer_blocks.7.norm2.bias",
        "backbone.transformer_blocks.7.attn2.to_q.weight",
        "backbone.transformer_blocks.7.attn2.to_k.weight",
        "backbone.transformer_blocks.7.attn2.to_v.weight",
        "backbone.transformer_blocks.7.attn2.to_out.0.weight",
        "backbone.transformer_blocks.7.attn2.to_out.0.bias",
        "backbone.transformer_blocks.7.norm3.weight",
        "backbone.transformer_blocks.7.norm3.bias",
        "backbone.transformer_blocks.7.ff.net.0.proj.weight",
        "backbone.transformer_blocks.7.ff.net.0.proj.bias",
        "backbone.transformer_blocks.7.ff.net.2.weight",
        "backbone.transformer_blocks.7.ff.net.2.bias",
        "backbone.transformer_blocks.8.norm1.weight",
        "backbone.transformer_blocks.8.norm1.bias",
        "backbone.transformer_blocks.8.attn1.to_q.weight",
        "backbone.transformer_blocks.8.attn1.to_k.weight",
        "backbone.transformer_blocks.8.attn1.to_v.weight",
        "backbone.transformer_blocks.8.attn1.to_out.0.weight",
        "backbone.transformer_blocks.8.attn1.to_out.0.bias",
        "backbone.transformer_blocks.8.norm2.weight",
        "backbone.transformer_blocks.8.norm2.bias",
        "backbone.transformer_blocks.8.attn2.to_q.weight",
        "backbone.transformer_blocks.8.attn2.to_k.weight",
        "backbone.transformer_blocks.8.attn2.to_v.weight",
        "backbone.transformer_blocks.8.attn2.to_out.0.weight",
        "backbone.transformer_blocks.8.attn2.to_out.0.bias",
        "backbone.transformer_blocks.8.norm3.weight",
        "backbone.transformer_blocks.8.norm3.bias",
        "backbone.transformer_blocks.8.ff.net.0.proj.weight",
        "backbone.transformer_blocks.8.ff.net.0.proj.bias",
        "backbone.transformer_blocks.8.ff.net.2.weight",
        "backbone.transformer_blocks.8.ff.net.2.bias",
        "backbone.transformer_blocks.9.norm1.weight",
        "backbone.transformer_blocks.9.norm1.bias",
        "backbone.transformer_blocks.9.attn1.to_q.weight",
        "backbone.transformer_blocks.9.attn1.to_k.weight",
        "backbone.transformer_blocks.9.attn1.to_v.weight",
        "backbone.transformer_blocks.9.attn1.to_out.0.weight",
        "backbone.transformer_blocks.9.attn1.to_out.0.bias",
        "backbone.transformer_blocks.9.norm2.weight",
        "backbone.transformer_blocks.9.norm2.bias",
        "backbone.transformer_blocks.9.attn2.to_q.weight",
        "backbone.transformer_blocks.9.attn2.to_k.weight",
        "backbone.transformer_blocks.9.attn2.to_v.weight",
        "backbone.transformer_blocks.9.attn2.to_out.0.weight",
        "backbone.transformer_blocks.9.attn2.to_out.0.bias",
        "backbone.transformer_blocks.9.norm3.weight",
        "backbone.transformer_blocks.9.norm3.bias",
        "backbone.transformer_blocks.9.ff.net.0.proj.weight",
        "backbone.transformer_blocks.9.ff.net.0.proj.bias",
        "backbone.transformer_blocks.9.ff.net.2.weight",
        "backbone.transformer_blocks.9.ff.net.2.bias",
        "backbone.transformer_blocks.10.norm1.weight",
        "backbone.transformer_blocks.10.norm1.bias",
        "backbone.transformer_blocks.10.attn1.to_q.weight",
        "backbone.transformer_blocks.10.attn1.to_k.weight",
        "backbone.transformer_blocks.10.attn1.to_v.weight",
        "backbone.transformer_blocks.10.attn1.to_out.0.weight",
        "backbone.transformer_blocks.10.attn1.to_out.0.bias",
        "backbone.transformer_blocks.10.norm2.weight",
        "backbone.transformer_blocks.10.norm2.bias",
        "backbone.transformer_blocks.10.attn2.to_q.weight",
        "backbone.transformer_blocks.10.attn2.to_k.weight",
        "backbone.transformer_blocks.10.attn2.to_v.weight",
        "backbone.transformer_blocks.10.attn2.to_out.0.weight",
        "backbone.transformer_blocks.10.attn2.to_out.0.bias",
        "backbone.transformer_blocks.10.norm3.weight",
        "backbone.transformer_blocks.10.norm3.bias",
        "backbone.transformer_blocks.10.ff.net.0.proj.weight",
        "backbone.transformer_blocks.10.ff.net.0.proj.bias",
        "backbone.transformer_blocks.10.ff.net.2.weight",
        "backbone.transformer_blocks.10.ff.net.2.bias",
        "backbone.transformer_blocks.11.norm1.weight",
        "backbone.transformer_blocks.11.norm1.bias",
        "backbone.transformer_blocks.11.attn1.to_q.weight",
        "backbone.transformer_blocks.11.attn1.to_k.weight",
        "backbone.transformer_blocks.11.attn1.to_v.weight",
        "backbone.transformer_blocks.11.attn1.to_out.0.weight",
        "backbone.transformer_blocks.11.attn1.to_out.0.bias",
        "backbone.transformer_blocks.11.norm2.weight",
        "backbone.transformer_blocks.11.norm2.bias",
        "backbone.transformer_blocks.11.attn2.to_q.weight",
        "backbone.transformer_blocks.11.attn2.to_k.weight",
        "backbone.transformer_blocks.11.attn2.to_v.weight",
        "backbone.transformer_blocks.11.attn2.to_out.0.weight",
        "backbone.transformer_blocks.11.attn2.to_out.0.bias",
        "backbone.transformer_blocks.11.norm3.weight",
        "backbone.transformer_blocks.11.norm3.bias",
        "backbone.transformer_blocks.11.ff.net.0.proj.weight",
        "backbone.transformer_blocks.11.ff.net.0.proj.bias",
        "backbone.transformer_blocks.11.ff.net.2.weight",
        "backbone.transformer_blocks.11.ff.net.2.bias",
        "backbone.transformer_blocks.12.norm1.weight",
        "backbone.transformer_blocks.12.norm1.bias",
        "backbone.transformer_blocks.12.attn1.to_q.weight",
        "backbone.transformer_blocks.12.attn1.to_k.weight",
        "backbone.transformer_blocks.12.attn1.to_v.weight",
        "backbone.transformer_blocks.12.attn1.to_out.0.weight",
        "backbone.transformer_blocks.12.attn1.to_out.0.bias",
        "backbone.transformer_blocks.12.norm2.weight",
        "backbone.transformer_blocks.12.norm2.bias",
        "backbone.transformer_blocks.12.attn2.to_q.weight",
        "backbone.transformer_blocks.12.attn2.to_k.weight",
        "backbone.transformer_blocks.12.attn2.to_v.weight",
        "backbone.transformer_blocks.12.attn2.to_out.0.weight",
        "backbone.transformer_blocks.12.attn2.to_out.0.bias",
        "backbone.transformer_blocks.12.norm3.weight",
        "backbone.transformer_blocks.12.norm3.bias",
        "backbone.transformer_blocks.12.ff.net.0.proj.weight",
        "backbone.transformer_blocks.12.ff.net.0.proj.bias",
        "backbone.transformer_blocks.12.ff.net.2.weight",
        "backbone.transformer_blocks.12.ff.net.2.bias",
        "backbone.transformer_blocks.13.norm1.weight",
        "backbone.transformer_blocks.13.norm1.bias",
        "backbone.transformer_blocks.13.attn1.to_q.weight",
        "backbone.transformer_blocks.13.attn1.to_k.weight",
        "backbone.transformer_blocks.13.attn1.to_v.weight",
        "backbone.transformer_blocks.13.attn1.to_out.0.weight",
        "backbone.transformer_blocks.13.attn1.to_out.0.bias",
        "backbone.transformer_blocks.13.norm2.weight",
        "backbone.transformer_blocks.13.norm2.bias",
        "backbone.transformer_blocks.13.attn2.to_q.weight",
        "backbone.transformer_blocks.13.attn2.to_k.weight",
        "backbone.transformer_blocks.13.attn2.to_v.weight",
        "backbone.transformer_blocks.13.attn2.to_out.0.weight",
        "backbone.transformer_blocks.13.attn2.to_out.0.bias",
        "backbone.transformer_blocks.13.norm3.weight",
        "backbone.transformer_blocks.13.norm3.bias",
        "backbone.transformer_blocks.13.ff.net.0.proj.weight",
        "backbone.transformer_blocks.13.ff.net.0.proj.bias",
        "backbone.transformer_blocks.13.ff.net.2.weight",
        "backbone.transformer_blocks.13.ff.net.2.bias",
        "backbone.transformer_blocks.14.norm1.weight",
        "backbone.transformer_blocks.14.norm1.bias",
        "backbone.transformer_blocks.14.attn1.to_q.weight",
        "backbone.transformer_blocks.14.attn1.to_k.weight",
        "backbone.transformer_blocks.14.attn1.to_v.weight",
        "backbone.transformer_blocks.14.attn1.to_out.0.weight",
        "backbone.transformer_blocks.14.attn1.to_out.0.bias",
        "backbone.transformer_blocks.14.norm2.weight",
        "backbone.transformer_blocks.14.norm2.bias",
        "backbone.transformer_blocks.14.attn2.to_q.weight",
        "backbone.transformer_blocks.14.attn2.to_k.weight",
        "backbone.transformer_blocks.14.attn2.to_v.weight",
        "backbone.transformer_blocks.14.attn2.to_out.0.weight",
        "backbone.transformer_blocks.14.attn2.to_out.0.bias",
        "backbone.transformer_blocks.14.norm3.weight",
        "backbone.transformer_blocks.14.norm3.bias",
        "backbone.transformer_blocks.14.ff.net.0.proj.weight",
        "backbone.transformer_blocks.14.ff.net.0.proj.bias",
        "backbone.transformer_blocks.14.ff.net.2.weight",
        "backbone.transformer_blocks.14.ff.net.2.bias",
        "backbone.transformer_blocks.15.norm1.weight",
        "backbone.transformer_blocks.15.norm1.bias",
        "backbone.transformer_blocks.15.attn1.to_q.weight",
        "backbone.transformer_blocks.15.attn1.to_k.weight",
        "backbone.transformer_blocks.15.attn1.to_v.weight",
        "backbone.transformer_blocks.15.attn1.to_out.0.weight",
        "backbone.transformer_blocks.15.attn1.to_out.0.bias",
        "backbone.transformer_blocks.15.norm2.weight",
        "backbone.transformer_blocks.15.norm2.bias",
        "backbone.transformer_blocks.15.attn2.to_q.weight",
        "backbone.transformer_blocks.15.attn2.to_k.weight",
        "backbone.transformer_blocks.15.attn2.to_v.weight",
        "backbone.transformer_blocks.15.attn2.to_out.0.weight",
        "backbone.transformer_blocks.15.attn2.to_out.0.bias",
        "backbone.transformer_blocks.15.norm3.weight",
        "backbone.transformer_blocks.15.norm3.bias",
        "backbone.transformer_blocks.15.ff.net.0.proj.weight",
        "backbone.transformer_blocks.15.ff.net.0.proj.bias",
        "backbone.transformer_blocks.15.ff.net.2.weight",
        "backbone.transformer_blocks.15.ff.net.2.bias",
        "backbone.proj_out.weight",
        "backbone.proj_out.bias"
      ],
      "post_processor": [
        "post_processor.upsample.weight",
        "post_processor.upsample.bias"
      ],
      "decoder": [
        "decoder.layers.0.weight",
        "decoder.layers.0.bias",
        "decoder.layers.2.weight",
        "decoder.layers.2.bias",
        "decoder.layers.4.weight",
        "decoder.layers.4.bias",
        "decoder.layers.6.weight",
        "decoder.layers.6.bias",
        "decoder.layers.8.weight",
        "decoder.layers.8.bias",
        "decoder.layers.10.weight",
        "decoder.layers.10.bias",
        "decoder.layers.12.weight",
        "decoder.layers.12.bias",
        "decoder.layers.14.weight",
        "decoder.layers.14.bias",
        "decoder.layers.16.weight",
        "decoder.layers.16.bias",
        "decoder.layers.18.weight",
        "decoder.layers.18.bias"
      ]
    },
    "model_components": [
      "encoder",
      "decoder"
    ]
  },
  "summary": {
    "total_parameters": 419275628,
    "total_size_mb": 1599.4095916748047,
    "parameter_groups": {
      "image_tokenizer": {
        "parameters": {
          "image_tokenizer.model.embeddings.cls_token": {
            "shape": [
              1,
              1,
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.embeddings.position_embeddings": {
            "shape": [
              1,
              197,
              768
            ],
            "size": 151296
          },
          "image_tokenizer.model.embeddings.patch_embeddings.projection.weight": {
            "shape": [
              768,
              3,
              16,
              16
            ],
            "size": 589824
          },
          "image_tokenizer.model.embeddings.patch_embeddings.projection.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.0.attention.attention.query.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.0.attention.attention.query.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.0.attention.attention.key.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.0.attention.attention.key.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.0.attention.attention.value.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.0.attention.attention.value.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.0.attention.output.dense.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.0.attention.output.dense.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.0.intermediate.dense.weight": {
            "shape": [
              3072,
              768
            ],
            "size": 2359296
          },
          "image_tokenizer.model.encoder.layer.0.intermediate.dense.bias": {
            "shape": [
              3072
            ],
            "size": 3072
          },
          "image_tokenizer.model.encoder.layer.0.output.dense.weight": {
            "shape": [
              768,
              3072
            ],
            "size": 2359296
          },
          "image_tokenizer.model.encoder.layer.0.output.dense.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.0.layernorm_before.weight": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.0.layernorm_before.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.0.layernorm_after.weight": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.0.layernorm_after.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.1.attention.attention.query.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.1.attention.attention.query.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.1.attention.attention.key.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.1.attention.attention.key.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.1.attention.attention.value.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.1.attention.attention.value.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.1.attention.output.dense.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.1.attention.output.dense.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.1.intermediate.dense.weight": {
            "shape": [
              3072,
              768
            ],
            "size": 2359296
          },
          "image_tokenizer.model.encoder.layer.1.intermediate.dense.bias": {
            "shape": [
              3072
            ],
            "size": 3072
          },
          "image_tokenizer.model.encoder.layer.1.output.dense.weight": {
            "shape": [
              768,
              3072
            ],
            "size": 2359296
          },
          "image_tokenizer.model.encoder.layer.1.output.dense.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.1.layernorm_before.weight": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.1.layernorm_before.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.1.layernorm_after.weight": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.1.layernorm_after.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.2.attention.attention.query.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.2.attention.attention.query.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.2.attention.attention.key.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.2.attention.attention.key.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.2.attention.attention.value.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.2.attention.attention.value.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.2.attention.output.dense.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.2.attention.output.dense.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.2.intermediate.dense.weight": {
            "shape": [
              3072,
              768
            ],
            "size": 2359296
          },
          "image_tokenizer.model.encoder.layer.2.intermediate.dense.bias": {
            "shape": [
              3072
            ],
            "size": 3072
          },
          "image_tokenizer.model.encoder.layer.2.output.dense.weight": {
            "shape": [
              768,
              3072
            ],
            "size": 2359296
          },
          "image_tokenizer.model.encoder.layer.2.output.dense.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.2.layernorm_before.weight": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.2.layernorm_before.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.2.layernorm_after.weight": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.2.layernorm_after.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.3.attention.attention.query.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.3.attention.attention.query.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.3.attention.attention.key.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.3.attention.attention.key.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.3.attention.attention.value.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.3.attention.attention.value.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.3.attention.output.dense.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.3.attention.output.dense.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.3.intermediate.dense.weight": {
            "shape": [
              3072,
              768
            ],
            "size": 2359296
          },
          "image_tokenizer.model.encoder.layer.3.intermediate.dense.bias": {
            "shape": [
              3072
            ],
            "size": 3072
          },
          "image_tokenizer.model.encoder.layer.3.output.dense.weight": {
            "shape": [
              768,
              3072
            ],
            "size": 2359296
          },
          "image_tokenizer.model.encoder.layer.3.output.dense.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.3.layernorm_before.weight": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.3.layernorm_before.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.3.layernorm_after.weight": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.3.layernorm_after.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.4.attention.attention.query.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.4.attention.attention.query.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.4.attention.attention.key.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.4.attention.attention.key.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.4.attention.attention.value.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.4.attention.attention.value.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.4.attention.output.dense.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.4.attention.output.dense.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.4.intermediate.dense.weight": {
            "shape": [
              3072,
              768
            ],
            "size": 2359296
          },
          "image_tokenizer.model.encoder.layer.4.intermediate.dense.bias": {
            "shape": [
              3072
            ],
            "size": 3072
          },
          "image_tokenizer.model.encoder.layer.4.output.dense.weight": {
            "shape": [
              768,
              3072
            ],
            "size": 2359296
          },
          "image_tokenizer.model.encoder.layer.4.output.dense.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.4.layernorm_before.weight": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.4.layernorm_before.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.4.layernorm_after.weight": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.4.layernorm_after.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.5.attention.attention.query.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.5.attention.attention.query.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.5.attention.attention.key.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.5.attention.attention.key.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.5.attention.attention.value.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.5.attention.attention.value.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.5.attention.output.dense.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.5.attention.output.dense.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.5.intermediate.dense.weight": {
            "shape": [
              3072,
              768
            ],
            "size": 2359296
          },
          "image_tokenizer.model.encoder.layer.5.intermediate.dense.bias": {
            "shape": [
              3072
            ],
            "size": 3072
          },
          "image_tokenizer.model.encoder.layer.5.output.dense.weight": {
            "shape": [
              768,
              3072
            ],
            "size": 2359296
          },
          "image_tokenizer.model.encoder.layer.5.output.dense.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.5.layernorm_before.weight": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.5.layernorm_before.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.5.layernorm_after.weight": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.5.layernorm_after.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.6.attention.attention.query.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.6.attention.attention.query.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.6.attention.attention.key.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.6.attention.attention.key.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.6.attention.attention.value.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.6.attention.attention.value.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.6.attention.output.dense.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.6.attention.output.dense.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.6.intermediate.dense.weight": {
            "shape": [
              3072,
              768
            ],
            "size": 2359296
          },
          "image_tokenizer.model.encoder.layer.6.intermediate.dense.bias": {
            "shape": [
              3072
            ],
            "size": 3072
          },
          "image_tokenizer.model.encoder.layer.6.output.dense.weight": {
            "shape": [
              768,
              3072
            ],
            "size": 2359296
          },
          "image_tokenizer.model.encoder.layer.6.output.dense.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.6.layernorm_before.weight": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.6.layernorm_before.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.6.layernorm_after.weight": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.6.layernorm_after.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.7.attention.attention.query.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.7.attention.attention.query.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.7.attention.attention.key.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.7.attention.attention.key.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.7.attention.attention.value.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.7.attention.attention.value.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.7.attention.output.dense.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.7.attention.output.dense.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.7.intermediate.dense.weight": {
            "shape": [
              3072,
              768
            ],
            "size": 2359296
          },
          "image_tokenizer.model.encoder.layer.7.intermediate.dense.bias": {
            "shape": [
              3072
            ],
            "size": 3072
          },
          "image_tokenizer.model.encoder.layer.7.output.dense.weight": {
            "shape": [
              768,
              3072
            ],
            "size": 2359296
          },
          "image_tokenizer.model.encoder.layer.7.output.dense.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.7.layernorm_before.weight": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.7.layernorm_before.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.7.layernorm_after.weight": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.7.layernorm_after.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.8.attention.attention.query.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.8.attention.attention.query.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.8.attention.attention.key.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.8.attention.attention.key.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.8.attention.attention.value.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.8.attention.attention.value.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.8.attention.output.dense.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.8.attention.output.dense.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.8.intermediate.dense.weight": {
            "shape": [
              3072,
              768
            ],
            "size": 2359296
          },
          "image_tokenizer.model.encoder.layer.8.intermediate.dense.bias": {
            "shape": [
              3072
            ],
            "size": 3072
          },
          "image_tokenizer.model.encoder.layer.8.output.dense.weight": {
            "shape": [
              768,
              3072
            ],
            "size": 2359296
          },
          "image_tokenizer.model.encoder.layer.8.output.dense.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.8.layernorm_before.weight": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.8.layernorm_before.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.8.layernorm_after.weight": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.8.layernorm_after.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.9.attention.attention.query.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.9.attention.attention.query.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.9.attention.attention.key.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.9.attention.attention.key.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.9.attention.attention.value.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.9.attention.attention.value.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.9.attention.output.dense.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.9.attention.output.dense.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.9.intermediate.dense.weight": {
            "shape": [
              3072,
              768
            ],
            "size": 2359296
          },
          "image_tokenizer.model.encoder.layer.9.intermediate.dense.bias": {
            "shape": [
              3072
            ],
            "size": 3072
          },
          "image_tokenizer.model.encoder.layer.9.output.dense.weight": {
            "shape": [
              768,
              3072
            ],
            "size": 2359296
          },
          "image_tokenizer.model.encoder.layer.9.output.dense.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.9.layernorm_before.weight": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.9.layernorm_before.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.9.layernorm_after.weight": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.9.layernorm_after.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.10.attention.attention.query.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.10.attention.attention.query.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.10.attention.attention.key.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.10.attention.attention.key.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.10.attention.attention.value.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.10.attention.attention.value.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.10.attention.output.dense.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.10.attention.output.dense.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.10.intermediate.dense.weight": {
            "shape": [
              3072,
              768
            ],
            "size": 2359296
          },
          "image_tokenizer.model.encoder.layer.10.intermediate.dense.bias": {
            "shape": [
              3072
            ],
            "size": 3072
          },
          "image_tokenizer.model.encoder.layer.10.output.dense.weight": {
            "shape": [
              768,
              3072
            ],
            "size": 2359296
          },
          "image_tokenizer.model.encoder.layer.10.output.dense.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.10.layernorm_before.weight": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.10.layernorm_before.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.10.layernorm_after.weight": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.10.layernorm_after.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.11.attention.attention.query.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.11.attention.attention.query.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.11.attention.attention.key.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.11.attention.attention.key.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.11.attention.attention.value.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.11.attention.attention.value.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.11.attention.output.dense.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.encoder.layer.11.attention.output.dense.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.11.intermediate.dense.weight": {
            "shape": [
              3072,
              768
            ],
            "size": 2359296
          },
          "image_tokenizer.model.encoder.layer.11.intermediate.dense.bias": {
            "shape": [
              3072
            ],
            "size": 3072
          },
          "image_tokenizer.model.encoder.layer.11.output.dense.weight": {
            "shape": [
              768,
              3072
            ],
            "size": 2359296
          },
          "image_tokenizer.model.encoder.layer.11.output.dense.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.11.layernorm_before.weight": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.11.layernorm_before.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.11.layernorm_after.weight": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.encoder.layer.11.layernorm_after.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.layernorm.weight": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.layernorm.bias": {
            "shape": [
              768
            ],
            "size": 768
          },
          "image_tokenizer.model.pooler.dense.weight": {
            "shape": [
              768,
              768
            ],
            "size": 589824
          },
          "image_tokenizer.model.pooler.dense.bias": {
            "shape": [
              768
            ],
            "size": 768
          }
        },
        "count": 200,
        "total_params": 86389248
      },
      "tokenizer": {
        "parameters": {
          "tokenizer.embeddings": {
            "shape": [
              3,
              1024,
              32,
              32
            ],
            "size": 3145728
          }
        },
        "count": 1,
        "total_params": 3145728
      },
      "backbone": {
        "parameters": {
          "backbone.norm.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.norm.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.proj_in.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.proj_in.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.0.norm1.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.0.norm1.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.0.attn1.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.0.attn1.to_k.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.0.attn1.to_v.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.0.attn1.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.0.attn1.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.0.norm2.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.0.norm2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.0.attn2.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.0.attn2.to_k.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.0.attn2.to_v.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.0.attn2.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.0.attn2.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.0.norm3.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.0.norm3.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.0.ff.net.0.proj.weight": {
            "shape": [
              8192,
              1024
            ],
            "size": 8388608
          },
          "backbone.transformer_blocks.0.ff.net.0.proj.bias": {
            "shape": [
              8192
            ],
            "size": 8192
          },
          "backbone.transformer_blocks.0.ff.net.2.weight": {
            "shape": [
              1024,
              4096
            ],
            "size": 4194304
          },
          "backbone.transformer_blocks.0.ff.net.2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.1.norm1.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.1.norm1.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.1.attn1.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.1.attn1.to_k.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.1.attn1.to_v.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.1.attn1.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.1.attn1.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.1.norm2.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.1.norm2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.1.attn2.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.1.attn2.to_k.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.1.attn2.to_v.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.1.attn2.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.1.attn2.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.1.norm3.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.1.norm3.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.1.ff.net.0.proj.weight": {
            "shape": [
              8192,
              1024
            ],
            "size": 8388608
          },
          "backbone.transformer_blocks.1.ff.net.0.proj.bias": {
            "shape": [
              8192
            ],
            "size": 8192
          },
          "backbone.transformer_blocks.1.ff.net.2.weight": {
            "shape": [
              1024,
              4096
            ],
            "size": 4194304
          },
          "backbone.transformer_blocks.1.ff.net.2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.2.norm1.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.2.norm1.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.2.attn1.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.2.attn1.to_k.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.2.attn1.to_v.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.2.attn1.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.2.attn1.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.2.norm2.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.2.norm2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.2.attn2.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.2.attn2.to_k.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.2.attn2.to_v.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.2.attn2.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.2.attn2.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.2.norm3.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.2.norm3.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.2.ff.net.0.proj.weight": {
            "shape": [
              8192,
              1024
            ],
            "size": 8388608
          },
          "backbone.transformer_blocks.2.ff.net.0.proj.bias": {
            "shape": [
              8192
            ],
            "size": 8192
          },
          "backbone.transformer_blocks.2.ff.net.2.weight": {
            "shape": [
              1024,
              4096
            ],
            "size": 4194304
          },
          "backbone.transformer_blocks.2.ff.net.2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.3.norm1.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.3.norm1.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.3.attn1.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.3.attn1.to_k.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.3.attn1.to_v.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.3.attn1.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.3.attn1.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.3.norm2.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.3.norm2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.3.attn2.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.3.attn2.to_k.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.3.attn2.to_v.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.3.attn2.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.3.attn2.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.3.norm3.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.3.norm3.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.3.ff.net.0.proj.weight": {
            "shape": [
              8192,
              1024
            ],
            "size": 8388608
          },
          "backbone.transformer_blocks.3.ff.net.0.proj.bias": {
            "shape": [
              8192
            ],
            "size": 8192
          },
          "backbone.transformer_blocks.3.ff.net.2.weight": {
            "shape": [
              1024,
              4096
            ],
            "size": 4194304
          },
          "backbone.transformer_blocks.3.ff.net.2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.4.norm1.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.4.norm1.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.4.attn1.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.4.attn1.to_k.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.4.attn1.to_v.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.4.attn1.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.4.attn1.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.4.norm2.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.4.norm2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.4.attn2.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.4.attn2.to_k.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.4.attn2.to_v.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.4.attn2.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.4.attn2.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.4.norm3.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.4.norm3.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.4.ff.net.0.proj.weight": {
            "shape": [
              8192,
              1024
            ],
            "size": 8388608
          },
          "backbone.transformer_blocks.4.ff.net.0.proj.bias": {
            "shape": [
              8192
            ],
            "size": 8192
          },
          "backbone.transformer_blocks.4.ff.net.2.weight": {
            "shape": [
              1024,
              4096
            ],
            "size": 4194304
          },
          "backbone.transformer_blocks.4.ff.net.2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.5.norm1.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.5.norm1.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.5.attn1.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.5.attn1.to_k.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.5.attn1.to_v.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.5.attn1.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.5.attn1.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.5.norm2.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.5.norm2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.5.attn2.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.5.attn2.to_k.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.5.attn2.to_v.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.5.attn2.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.5.attn2.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.5.norm3.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.5.norm3.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.5.ff.net.0.proj.weight": {
            "shape": [
              8192,
              1024
            ],
            "size": 8388608
          },
          "backbone.transformer_blocks.5.ff.net.0.proj.bias": {
            "shape": [
              8192
            ],
            "size": 8192
          },
          "backbone.transformer_blocks.5.ff.net.2.weight": {
            "shape": [
              1024,
              4096
            ],
            "size": 4194304
          },
          "backbone.transformer_blocks.5.ff.net.2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.6.norm1.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.6.norm1.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.6.attn1.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.6.attn1.to_k.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.6.attn1.to_v.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.6.attn1.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.6.attn1.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.6.norm2.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.6.norm2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.6.attn2.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.6.attn2.to_k.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.6.attn2.to_v.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.6.attn2.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.6.attn2.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.6.norm3.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.6.norm3.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.6.ff.net.0.proj.weight": {
            "shape": [
              8192,
              1024
            ],
            "size": 8388608
          },
          "backbone.transformer_blocks.6.ff.net.0.proj.bias": {
            "shape": [
              8192
            ],
            "size": 8192
          },
          "backbone.transformer_blocks.6.ff.net.2.weight": {
            "shape": [
              1024,
              4096
            ],
            "size": 4194304
          },
          "backbone.transformer_blocks.6.ff.net.2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.7.norm1.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.7.norm1.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.7.attn1.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.7.attn1.to_k.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.7.attn1.to_v.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.7.attn1.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.7.attn1.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.7.norm2.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.7.norm2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.7.attn2.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.7.attn2.to_k.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.7.attn2.to_v.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.7.attn2.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.7.attn2.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.7.norm3.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.7.norm3.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.7.ff.net.0.proj.weight": {
            "shape": [
              8192,
              1024
            ],
            "size": 8388608
          },
          "backbone.transformer_blocks.7.ff.net.0.proj.bias": {
            "shape": [
              8192
            ],
            "size": 8192
          },
          "backbone.transformer_blocks.7.ff.net.2.weight": {
            "shape": [
              1024,
              4096
            ],
            "size": 4194304
          },
          "backbone.transformer_blocks.7.ff.net.2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.8.norm1.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.8.norm1.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.8.attn1.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.8.attn1.to_k.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.8.attn1.to_v.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.8.attn1.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.8.attn1.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.8.norm2.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.8.norm2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.8.attn2.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.8.attn2.to_k.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.8.attn2.to_v.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.8.attn2.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.8.attn2.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.8.norm3.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.8.norm3.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.8.ff.net.0.proj.weight": {
            "shape": [
              8192,
              1024
            ],
            "size": 8388608
          },
          "backbone.transformer_blocks.8.ff.net.0.proj.bias": {
            "shape": [
              8192
            ],
            "size": 8192
          },
          "backbone.transformer_blocks.8.ff.net.2.weight": {
            "shape": [
              1024,
              4096
            ],
            "size": 4194304
          },
          "backbone.transformer_blocks.8.ff.net.2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.9.norm1.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.9.norm1.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.9.attn1.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.9.attn1.to_k.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.9.attn1.to_v.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.9.attn1.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.9.attn1.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.9.norm2.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.9.norm2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.9.attn2.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.9.attn2.to_k.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.9.attn2.to_v.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.9.attn2.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.9.attn2.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.9.norm3.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.9.norm3.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.9.ff.net.0.proj.weight": {
            "shape": [
              8192,
              1024
            ],
            "size": 8388608
          },
          "backbone.transformer_blocks.9.ff.net.0.proj.bias": {
            "shape": [
              8192
            ],
            "size": 8192
          },
          "backbone.transformer_blocks.9.ff.net.2.weight": {
            "shape": [
              1024,
              4096
            ],
            "size": 4194304
          },
          "backbone.transformer_blocks.9.ff.net.2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.10.norm1.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.10.norm1.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.10.attn1.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.10.attn1.to_k.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.10.attn1.to_v.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.10.attn1.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.10.attn1.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.10.norm2.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.10.norm2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.10.attn2.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.10.attn2.to_k.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.10.attn2.to_v.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.10.attn2.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.10.attn2.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.10.norm3.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.10.norm3.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.10.ff.net.0.proj.weight": {
            "shape": [
              8192,
              1024
            ],
            "size": 8388608
          },
          "backbone.transformer_blocks.10.ff.net.0.proj.bias": {
            "shape": [
              8192
            ],
            "size": 8192
          },
          "backbone.transformer_blocks.10.ff.net.2.weight": {
            "shape": [
              1024,
              4096
            ],
            "size": 4194304
          },
          "backbone.transformer_blocks.10.ff.net.2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.11.norm1.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.11.norm1.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.11.attn1.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.11.attn1.to_k.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.11.attn1.to_v.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.11.attn1.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.11.attn1.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.11.norm2.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.11.norm2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.11.attn2.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.11.attn2.to_k.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.11.attn2.to_v.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.11.attn2.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.11.attn2.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.11.norm3.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.11.norm3.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.11.ff.net.0.proj.weight": {
            "shape": [
              8192,
              1024
            ],
            "size": 8388608
          },
          "backbone.transformer_blocks.11.ff.net.0.proj.bias": {
            "shape": [
              8192
            ],
            "size": 8192
          },
          "backbone.transformer_blocks.11.ff.net.2.weight": {
            "shape": [
              1024,
              4096
            ],
            "size": 4194304
          },
          "backbone.transformer_blocks.11.ff.net.2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.12.norm1.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.12.norm1.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.12.attn1.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.12.attn1.to_k.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.12.attn1.to_v.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.12.attn1.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.12.attn1.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.12.norm2.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.12.norm2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.12.attn2.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.12.attn2.to_k.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.12.attn2.to_v.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.12.attn2.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.12.attn2.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.12.norm3.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.12.norm3.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.12.ff.net.0.proj.weight": {
            "shape": [
              8192,
              1024
            ],
            "size": 8388608
          },
          "backbone.transformer_blocks.12.ff.net.0.proj.bias": {
            "shape": [
              8192
            ],
            "size": 8192
          },
          "backbone.transformer_blocks.12.ff.net.2.weight": {
            "shape": [
              1024,
              4096
            ],
            "size": 4194304
          },
          "backbone.transformer_blocks.12.ff.net.2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.13.norm1.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.13.norm1.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.13.attn1.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.13.attn1.to_k.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.13.attn1.to_v.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.13.attn1.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.13.attn1.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.13.norm2.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.13.norm2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.13.attn2.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.13.attn2.to_k.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.13.attn2.to_v.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.13.attn2.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.13.attn2.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.13.norm3.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.13.norm3.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.13.ff.net.0.proj.weight": {
            "shape": [
              8192,
              1024
            ],
            "size": 8388608
          },
          "backbone.transformer_blocks.13.ff.net.0.proj.bias": {
            "shape": [
              8192
            ],
            "size": 8192
          },
          "backbone.transformer_blocks.13.ff.net.2.weight": {
            "shape": [
              1024,
              4096
            ],
            "size": 4194304
          },
          "backbone.transformer_blocks.13.ff.net.2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.14.norm1.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.14.norm1.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.14.attn1.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.14.attn1.to_k.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.14.attn1.to_v.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.14.attn1.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.14.attn1.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.14.norm2.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.14.norm2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.14.attn2.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.14.attn2.to_k.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.14.attn2.to_v.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.14.attn2.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.14.attn2.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.14.norm3.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.14.norm3.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.14.ff.net.0.proj.weight": {
            "shape": [
              8192,
              1024
            ],
            "size": 8388608
          },
          "backbone.transformer_blocks.14.ff.net.0.proj.bias": {
            "shape": [
              8192
            ],
            "size": 8192
          },
          "backbone.transformer_blocks.14.ff.net.2.weight": {
            "shape": [
              1024,
              4096
            ],
            "size": 4194304
          },
          "backbone.transformer_blocks.14.ff.net.2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.15.norm1.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.15.norm1.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.15.attn1.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.15.attn1.to_k.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.15.attn1.to_v.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.15.attn1.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.15.attn1.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.15.norm2.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.15.norm2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.15.attn2.to_q.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.15.attn2.to_k.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.15.attn2.to_v.weight": {
            "shape": [
              1024,
              768
            ],
            "size": 786432
          },
          "backbone.transformer_blocks.15.attn2.to_out.0.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.transformer_blocks.15.attn2.to_out.0.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.15.norm3.weight": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.15.norm3.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.transformer_blocks.15.ff.net.0.proj.weight": {
            "shape": [
              8192,
              1024
            ],
            "size": 8388608
          },
          "backbone.transformer_blocks.15.ff.net.0.proj.bias": {
            "shape": [
              8192
            ],
            "size": 8192
          },
          "backbone.transformer_blocks.15.ff.net.2.weight": {
            "shape": [
              1024,
              4096
            ],
            "size": 4194304
          },
          "backbone.transformer_blocks.15.ff.net.2.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          },
          "backbone.proj_out.weight": {
            "shape": [
              1024,
              1024
            ],
            "size": 1048576
          },
          "backbone.proj_out.bias": {
            "shape": [
              1024
            ],
            "size": 1024
          }
        },
        "count": 326,
        "total_params": 329535488
      },
      "post_processor": {
        "parameters": {
          "post_processor.upsample.weight": {
            "shape": [
              1024,
              40,
              2,
              2
            ],
            "size": 163840
          },
          "post_processor.upsample.bias": {
            "shape": [
              40
            ],
            "size": 40
          }
        },
        "count": 2,
        "total_params": 163880
      },
      "decoder": {
        "parameters": {
          "decoder.layers.0.weight": {
            "shape": [
              64,
              120
            ],
            "size": 7680
          },
          "decoder.layers.0.bias": {
            "shape": [
              64
            ],
            "size": 64
          },
          "decoder.layers.2.weight": {
            "shape": [
              64,
              64
            ],
            "size": 4096
          },
          "decoder.layers.2.bias": {
            "shape": [
              64
            ],
            "size": 64
          },
          "decoder.layers.4.weight": {
            "shape": [
              64,
              64
            ],
            "size": 4096
          },
          "decoder.layers.4.bias": {
            "shape": [
              64
            ],
            "size": 64
          },
          "decoder.layers.6.weight": {
            "shape": [
              64,
              64
            ],
            "size": 4096
          },
          "decoder.layers.6.bias": {
            "shape": [
              64
            ],
            "size": 64
          },
          "decoder.layers.8.weight": {
            "shape": [
              64,
              64
            ],
            "size": 4096
          },
          "decoder.layers.8.bias": {
            "shape": [
              64
            ],
            "size": 64
          },
          "decoder.layers.10.weight": {
            "shape": [
              64,
              64
            ],
            "size": 4096
          },
          "decoder.layers.10.bias": {
            "shape": [
              64
            ],
            "size": 64
          },
          "decoder.layers.12.weight": {
            "shape": [
              64,
              64
            ],
            "size": 4096
          },
          "decoder.layers.12.bias": {
            "shape": [
              64
            ],
            "size": 64
          },
          "decoder.layers.14.weight": {
            "shape": [
              64,
              64
            ],
            "size": 4096
          },
          "decoder.layers.14.bias": {
            "shape": [
              64
            ],
            "size": 64
          },
          "decoder.layers.16.weight": {
            "shape": [
              64,
              64
            ],
            "size": 4096
          },
          "decoder.layers.16.bias": {
            "shape": [
              64
            ],
            "size": 64
          },
          "decoder.layers.18.weight": {
            "shape": [
              4,
              64
            ],
            "size": 256
          },
          "decoder.layers.18.bias": {
            "shape": [
              4
            ],
            "size": 4
          }
        },
        "count": 20,
        "total_params": 41284
      }
    }
  }
}